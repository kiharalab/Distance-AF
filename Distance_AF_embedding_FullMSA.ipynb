{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiharalab/Distance-AF/blob/main/Distance_AF_embedding_FullMSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GhadJyq4FqT"
      },
      "source": [
        "# Distance-AF_embedding\n",
        "This notebook is about modified [AlphaFold colab version](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) as a tool to generate embeddings for running Distance-AF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAjTWTHvZnlv"
      },
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "**Quick start**\n",
        "1. Paste your protein sequence in the input field, current Distance-AF version only supports single chain.\n",
        "2. Press \"Runtime\" -> \"Run all\".\n",
        "3. The pipeline consists of 5 steps. The currently running step is indicated by a circle with a stop sign next to it.\n",
        "4. If you have trouble with connecting GPU, you can simply click the Runtime and scroll down to click 'change runtime type', at the section of 'Hardware accelerator', you can choose the avaliable GPU provided, finally click save.\n",
        "\n",
        "**Detail Instructions of AlphaFold part\n",
        "- Please check [AlphaFold](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb)\n",
        "\n",
        "**License**\n",
        "The source code of Distance-AF is licensed under GPL v3. (If you are interested in a different license, for example, for commercial use, please contact [us](dkihara@purdue.edu).)\n",
        "\n",
        "The source code of AlphaFold2 is licensed under [Apache v2.0](https://github.com/google-deepmind/alphafold/blob/main/LICENSE) and [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively. Read more about the AlphaFold license [here](https://github.com/deepmind/alphafold)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrlrDOLCjQbV"
      },
      "source": [
        "# Setup\n",
        "Start by running the 2 cells below to set up AlphaFold and all required software.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWfiTxKF4O4U",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Set environment variables before running any other code.\n",
        "import os\n",
        "os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '4.0'\n",
        "\n",
        "#@title 1. Install third-party software\n",
        "\n",
        "#@markdown Please execute this cell by pressing the _Play_ button\n",
        "#@markdown on the left to download and import third-party software\n",
        "#@markdown in this Colab notebook. (See the [acknowledgements](https://github.com/deepmind/alphafold/#acknowledgements) in our readme.)\n",
        "\n",
        "#@markdown **Note**: This installs the software on the Colab\n",
        "#@markdown notebook in the cloud and not on your computer.\n",
        "\n",
        "from IPython.utils import io\n",
        "import os\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      # Uninstall default Colab version of TF.\n",
        "      %shell pip uninstall -y tensorflow\n",
        "\n",
        "      %shell sudo apt install --quiet --yes hmmer\n",
        "      pbar.update(6)\n",
        "\n",
        "      # Install py3dmol.\n",
        "      %shell pip install py3dmol\n",
        "      pbar.update(2)\n",
        "\n",
        "      # Install OpenMM and pdbfixer.\n",
        "      %shell rm -rf /opt/conda\n",
        "      %shell wget -q -P /tmp \\\n",
        "        https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "          && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "          && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "      pbar.update(9)\n",
        "\n",
        "      PATH=%env PATH\n",
        "      %env PATH=/opt/conda/bin:{PATH}\n",
        "      %shell conda install -qy conda==23.5.2 \\\n",
        "          && conda install -qy -c conda-forge \\\n",
        "            python=3.10 \\\n",
        "            openmm=7.7.0 \\\n",
        "            pdbfixer\n",
        "      pbar.update(80)\n",
        "\n",
        "      # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "      %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "      %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "      pbar.update(2)\n",
        "\n",
        "      %shell wget -q -P /content \\\n",
        "        https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "      pbar.update(1)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "executed_cells = set([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m9REtyN4bs0"
      },
      "outputs": [],
      "source": [
        "#@title 2. Download AlphaFold\n",
        "\n",
        "#@markdown Please execute this cell by pressing the *Play* button on\n",
        "#@markdown the left.\n",
        "\n",
        "GIT_REPO = 'https://github.com/kiharalab/AF2_Distaf'\n",
        "SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-12-06.tar'\n",
        "PARAMS_DIR = './alphafold/data/params'\n",
        "PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      %shell rm -rf alphafold\n",
        "      %shell git clone --branch main {GIT_REPO} alphafold\n",
        "      pbar.update(8)\n",
        "      # Install the required versions of all dependencies.\n",
        "      %shell pip3 install -r ./alphafold/requirements.txt\n",
        "      # Run setup.py to install only AlphaFold.\n",
        "      %shell pip3 install --no-dependencies ./alphafold\n",
        "      %shell pip3 install pyopenssl==22.0.0\n",
        "      %shell sudo python -m pip uninstall keras -y\n",
        "      pbar.update(10)\n",
        "\n",
        "      # Make sure stereo_chemical_props.txt is in all locations where it could be searched for.\n",
        "      %shell mkdir -p /content/alphafold/alphafold/common\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/alphafold/common\n",
        "      %shell mkdir -p /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "\n",
        "      # Load parameters\n",
        "      %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "      %shell wget -O \"{PARAMS_PATH}\" \"{SOURCE_URL}\"\n",
        "      pbar.update(27)\n",
        "\n",
        "      %shell tar --extract --verbose --file=\"{PARAMS_PATH}\" \\\n",
        "        --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "      %shell rm \"{PARAMS_PATH}\"\n",
        "      pbar.update(55)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "import jax\n",
        "if jax.local_devices()[0].platform == 'tpu':\n",
        "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "elif jax.local_devices()[0].platform == 'cpu':\n",
        "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "else:\n",
        "  print(f'Running with {jax.local_devices()[0].device_kind} GPU')\n",
        "\n",
        "# Make sure everything we need is on the path.\n",
        "import sys\n",
        "sys.path.append('/opt/conda/lib/python3.10/site-packages')\n",
        "sys.path.append('/content/alphafold')\n",
        "\n",
        "executed_cells.add(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSVipSer4ms-"
      },
      "source": [
        "# Generate embedding\n",
        "Please paste the sequence of your protein in the text box below, then run the remaining cells via Runtime > Run after. You can also run the cells individually by pressing the Play button on the left.\n",
        "\n",
        "Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrIg6g5x5eeS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 3. Enter the amino acid sequence(s) to fold ⬇️\n",
        "\n",
        "#@markdown Enter the custom job_name, which is also the download zip file name.\n",
        "\n",
        "#@markdown Enter the amino acid sequence(s) to fold:\n",
        "#@markdown * If you enter only a single sequence, the monomer model will be\n",
        "#@markdown used (unless you override this below).\n",
        "#@markdown * If you enter multiple sequences, the multimer model will be used.\n",
        "\n",
        "#@markdown Current version of Distance-AF only supports monomer target.\n",
        "\n",
        "from alphafold.notebooks import notebook_utils\n",
        "# Track cell execution to ensure correct order.\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 3)\n",
        "\n",
        "import enum\n",
        "\n",
        "@enum.unique\n",
        "class ModelType(enum.Enum):\n",
        "  MONOMER = 0\n",
        "  MULTIMER = 1\n",
        "job_name = 'test_1IXC_A'   #@param {type:\"string\"}\n",
        "sequence_1 = 'MEFRQLKYFIAVAEAGNMAAAAKRLHVSQPPITRQMQALEADLGVVLLERSHRGIELTAAGHAFLEDARRILELAGRSGDRSRAAARGDVGELSVAYFGTPIYRSLPLLLRAFLTSTPTATVSLTHMTKDEQVEGLLAGTIHVGFSRFFPRHPGIEIVNIAQEDLYLAVHRSQSGKFGKTCKLADLRAVELTLFPRGGRPSFADEVIGLFKHAGIEPRIARVVEDATAALALTMAGAASSIVPASVAAIRWPDIAFARIVGTRVKVPISCIFRKEKQPPILARFVEHVRRSAKD'  #@param {type:\"string\"}\n",
        "sequence_2 = ''  #@param {type:\"string\"}\n",
        "sequence_3 = ''  #@param {type:\"string\"}\n",
        "sequence_4 = ''  #@param {type:\"string\"}\n",
        "sequence_5 = ''  #@param {type:\"string\"}\n",
        "sequence_6 = ''  #@param {type:\"string\"}\n",
        "sequence_7 = ''  #@param {type:\"string\"}\n",
        "sequence_8 = ''  #@param {type:\"string\"}\n",
        "sequence_9 = ''  #@param {type:\"string\"}\n",
        "sequence_10 = ''  #@param {type:\"string\"}\n",
        "sequence_11 = ''  #@param {type:\"string\"}\n",
        "sequence_12 = ''  #@param {type:\"string\"}\n",
        "sequence_13 = ''  #@param {type:\"string\"}\n",
        "sequence_14 = ''  #@param {type:\"string\"}\n",
        "sequence_15 = ''  #@param {type:\"string\"}\n",
        "sequence_16 = ''  #@param {type:\"string\"}\n",
        "sequence_17 = ''  #@param {type:\"string\"}\n",
        "sequence_18 = ''  #@param {type:\"string\"}\n",
        "sequence_19 = ''  #@param {type:\"string\"}\n",
        "sequence_20 = ''  #@param {type:\"string\"}\n",
        "\n",
        "input_sequences = (\n",
        "    sequence_1, sequence_2, sequence_3, sequence_4, sequence_5,\n",
        "    sequence_6, sequence_7, sequence_8, sequence_9, sequence_10,\n",
        "    sequence_11, sequence_12, sequence_13, sequence_14, sequence_15,\n",
        "    sequence_16, sequence_17, sequence_18, sequence_19, sequence_20)\n",
        "\n",
        "MIN_PER_SEQUENCE_LENGTH = 16\n",
        "MAX_PER_SEQUENCE_LENGTH = 4000\n",
        "MAX_MONOMER_MODEL_LENGTH = 2500\n",
        "MAX_LENGTH = 4000\n",
        "MAX_VALIDATED_LENGTH = 3000\n",
        "\n",
        "#@markdown Select this checkbox to run the multimer model for a single sequence.\n",
        "#@markdown For proteins that are monomeric in their native form, or for very\n",
        "#@markdown large single chains you may get better accuracy and memory efficiency\n",
        "#@markdown by using the multimer model.\n",
        "#@markdown\n",
        "#@markdown\n",
        "#@markdown Due to improved memory efficiency the multimer model has a maximum\n",
        "#@markdown limit of 4000 residues, while the monomer model has a limit of 2500\n",
        "#@markdown residues.\n",
        "\n",
        "use_multimer_model_for_monomers = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Validate the input sequences.\n",
        "sequences = notebook_utils.clean_and_validate_input_sequences(\n",
        "    input_sequences=input_sequences,\n",
        "    min_sequence_length=MIN_PER_SEQUENCE_LENGTH,\n",
        "    max_sequence_length=MAX_PER_SEQUENCE_LENGTH)\n",
        "\n",
        "if len(sequences) == 1:\n",
        "  if use_multimer_model_for_monomers:\n",
        "    print('Using the multimer model for single-chain, as requested.')\n",
        "    model_type_to_use = ModelType.MULTIMER\n",
        "  else:\n",
        "    print('Using the single-chain model.')\n",
        "    model_type_to_use = ModelType.MONOMER\n",
        "else:\n",
        "  print(f'Using the multimer model with {len(sequences)} sequences.')\n",
        "  model_type_to_use = ModelType.MULTIMER\n",
        "\n",
        "# Check whether total length exceeds limit.\n",
        "total_sequence_length = sum([len(seq) for seq in sequences])\n",
        "if total_sequence_length > MAX_LENGTH:\n",
        "  raise ValueError('The total sequence length is too long: '\n",
        "                   f'{total_sequence_length}, while the maximum is '\n",
        "                   f'{MAX_LENGTH}.')\n",
        "\n",
        "# Check whether we exceed the monomer limit.\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  if len(sequences[0]) > MAX_MONOMER_MODEL_LENGTH:\n",
        "    raise ValueError(\n",
        "        f'Input sequence is too long: {len(sequences[0])} amino acids, while '\n",
        "        f'the maximum for the monomer model is {MAX_MONOMER_MODEL_LENGTH}. You may '\n",
        "        'be able to run this sequence with the multimer model by selecting the '\n",
        "        'use_multimer_model_for_monomers checkbox above.')\n",
        "\n",
        "if total_sequence_length > MAX_VALIDATED_LENGTH:\n",
        "  print('WARNING: The accuracy of the system has not been fully validated '\n",
        "        'above 3000 residues, and you may experience long running times or '\n",
        "        f'run out of memory. Total sequence length is {total_sequence_length} '\n",
        "        'residues.')\n",
        "\n",
        "executed_cells.add(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stST8Snqu011"
      },
      "outputs": [],
      "source": [
        "#@title 4. Search against genetic databases\n",
        "\n",
        "#@markdown Once this cell has been executed, you will see\n",
        "#@markdown statistics about the multiple sequence alignment\n",
        "#@markdown (MSA) that will be used by AlphaFold. In particular,\n",
        "#@markdown you’ll see how well each residue is covered by similar\n",
        "#@markdown sequences in the MSA.\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 4)\n",
        "\n",
        "# --- Python imports ---\n",
        "import collections\n",
        "import copy\n",
        "from concurrent import futures\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from urllib import request\n",
        "from google.colab import files\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "\n",
        "from alphafold.model import model\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "\n",
        "from alphafold.data import feature_processing\n",
        "from alphafold.data import msa_pairing\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.data import pipeline_multimer\n",
        "from alphafold.data.tools import jackhmmer\n",
        "\n",
        "from alphafold.common import confidence\n",
        "from alphafold.common import protein\n",
        "\n",
        "from alphafold.relax import relax\n",
        "from alphafold.relax import utils\n",
        "\n",
        "from IPython import display\n",
        "from ipywidgets import GridspecLayout\n",
        "from ipywidgets import Output\n",
        "\n",
        "# Color bands for visualizing plddt\n",
        "PLDDT_BANDS = [(0, 50, '#FF7D45'),\n",
        "               (50, 70, '#FFDB13'),\n",
        "               (70, 90, '#65CBF3'),\n",
        "               (90, 100, '#0053D6')]\n",
        "\n",
        "# --- Find the closest source ---\n",
        "test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2022_01.fasta.1'\n",
        "ex = futures.ThreadPoolExecutor(3)\n",
        "def fetch(source):\n",
        "  request.urlretrieve(test_url_pattern.format(source))\n",
        "  return source\n",
        "fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "source = None\n",
        "for f in futures.as_completed(fs):\n",
        "  source = f.result()\n",
        "  ex.shutdown()\n",
        "  break\n",
        "\n",
        "JACKHMMER_BINARY_PATH = '/usr/bin/jackhmmer'\n",
        "DB_ROOT_PATH = f'https://storage.googleapis.com/alphafold-colab{source}/latest/'\n",
        "# The z_value is the number of sequences in a database.\n",
        "MSA_DATABASES = [\n",
        "    {'db_name': 'uniref90',\n",
        "     'db_path': f'{DB_ROOT_PATH}uniref90_2022_01.fasta',\n",
        "     'num_streamed_chunks': 62,\n",
        "     'z_value': 144_113_457},\n",
        "    {'db_name': 'smallbfd',\n",
        "     'db_path': f'{DB_ROOT_PATH}bfd-first_non_consensus_sequences.fasta',\n",
        "     'num_streamed_chunks': 17,\n",
        "     'z_value': 65_984_053},\n",
        "    {'db_name': 'mgnify',\n",
        "     'db_path': f'{DB_ROOT_PATH}mgy_clusters_2022_05.fasta',\n",
        "     'num_streamed_chunks': 120,\n",
        "     'z_value': 623_796_864},\n",
        "]\n",
        "\n",
        "# Search UniProt and construct the all_seq features only for heteromers, not homomers.\n",
        "if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "  MSA_DATABASES.extend([\n",
        "      # Swiss-Prot and TrEMBL are concatenated together as UniProt.\n",
        "      {'db_name': 'uniprot',\n",
        "       'db_path': f'{DB_ROOT_PATH}uniprot_2021_04.fasta',\n",
        "       'num_streamed_chunks': 101,\n",
        "       'z_value': 225_013_025 + 565_928},\n",
        "  ])\n",
        "\n",
        "TOTAL_JACKHMMER_CHUNKS = sum([cfg['num_streamed_chunks'] for cfg in MSA_DATABASES])\n",
        "\n",
        "MAX_HITS = {\n",
        "    'uniref90': 10_000,\n",
        "    'smallbfd': 5_000,\n",
        "    'mgnify': 501,\n",
        "    'uniprot': 50_000,\n",
        "}\n",
        "\n",
        "\n",
        "def get_msa(sequences):\n",
        "  \"\"\"Searches for MSA for given sequences using chunked Jackhmmer search.\n",
        "\n",
        "  Args:\n",
        "    sequences: A list of sequences to search against all databases.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary mapping unique sequences to dicionaries mapping each database\n",
        "    to a list of  results, one for each chunk of the database.\n",
        "  \"\"\"\n",
        "  sequence_to_fasta_path = {}\n",
        "  # Deduplicate to not do redundant work for multiple copies of the same chain in homomers.\n",
        "  for sequence_index, sequence in enumerate(sorted(set(sequences)), 1):\n",
        "    fasta_path = f'target_{sequence_index:02d}.fasta'\n",
        "    with open(fasta_path, 'wt') as f:\n",
        "      f.write(f'>query\\n{sequence}')\n",
        "    sequence_to_fasta_path[sequence] = fasta_path\n",
        "\n",
        "  # Run the search against chunks of genetic databases (since the genetic\n",
        "  # databases don't fit in Colab disk).\n",
        "  raw_msa_results = {sequence: {} for sequence in sequence_to_fasta_path.keys()}\n",
        "  print('\\nGetting MSA for all sequences')\n",
        "  with tqdm.notebook.tqdm(total=TOTAL_JACKHMMER_CHUNKS, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    def jackhmmer_chunk_callback(i):\n",
        "      pbar.update(n=1)\n",
        "\n",
        "    for db_config in MSA_DATABASES:\n",
        "      db_name = db_config['db_name']\n",
        "      pbar.set_description(f'Searching {db_name}')\n",
        "      jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=JACKHMMER_BINARY_PATH,\n",
        "          database_path=db_config['db_path'],\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=db_config['num_streamed_chunks'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=db_config['z_value'])\n",
        "      # Query all unique sequences against each chunk of the database to prevent\n",
        "      # redunantly fetching each chunk for each unique sequence.\n",
        "      results = jackhmmer_runner.query_multiple(list(sequence_to_fasta_path.values()))\n",
        "      for sequence, result_for_sequence in zip(sequence_to_fasta_path.keys(), results):\n",
        "        raw_msa_results[sequence][db_name] = result_for_sequence\n",
        "\n",
        "  return raw_msa_results\n",
        "\n",
        "\n",
        "features_for_chain = {}\n",
        "raw_msa_results_for_sequence = get_msa(sequences)\n",
        "for sequence_index, sequence in enumerate(sequences, start=1):\n",
        "  raw_msa_results = copy.deepcopy(raw_msa_results_for_sequence[sequence])\n",
        "\n",
        "  # Extract the MSAs from the Stockholm files.\n",
        "  # NB: deduplication happens later in pipeline.make_msa_features.\n",
        "  single_chain_msas = []\n",
        "  uniprot_msa = None\n",
        "  for db_name, db_results in raw_msa_results.items():\n",
        "    merged_msa = notebook_utils.merge_chunked_msa(\n",
        "        results=db_results, max_hits=MAX_HITS.get(db_name))\n",
        "    if merged_msa.sequences and db_name != 'uniprot':\n",
        "      single_chain_msas.append(merged_msa)\n",
        "      msa_size = len(set(merged_msa.sequences))\n",
        "      print(f'{msa_size} unique sequences found in {db_name} for sequence {sequence_index}')\n",
        "    elif merged_msa.sequences and db_name == 'uniprot':\n",
        "      uniprot_msa = merged_msa\n",
        "\n",
        "  notebook_utils.show_msa_info(single_chain_msas=single_chain_msas, sequence_index=sequence_index)\n",
        "\n",
        "  # Turn the raw data into model features.\n",
        "  feature_dict = {}\n",
        "  feature_dict.update(pipeline.make_sequence_features(\n",
        "      sequence=sequence, description='query', num_res=len(sequence)))\n",
        "  feature_dict.update(pipeline.make_msa_features(msas=single_chain_msas))\n",
        "  # We don't use templates in AlphaFold Colab notebook, add only empty placeholder features.\n",
        "  feature_dict.update(notebook_utils.empty_placeholder_template_features(\n",
        "      num_templates=0, num_res=len(sequence)))\n",
        "\n",
        "  # Construct the all_seq features only for heteromers, not homomers.\n",
        "  if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "    valid_feats = msa_pairing.MSA_FEATURES + (\n",
        "        'msa_species_identifiers',\n",
        "    )\n",
        "    all_seq_features = {\n",
        "        f'{k}_all_seq': v for k, v in pipeline.make_msa_features([uniprot_msa]).items()\n",
        "        if k in valid_feats}\n",
        "    feature_dict.update(all_seq_features)\n",
        "\n",
        "  features_for_chain[protein.PDB_CHAIN_IDS[sequence_index - 1]] = feature_dict\n",
        "\n",
        "\n",
        "# Do further feature post-processing depending on the model type.\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  np_example = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
        "\n",
        "elif model_type_to_use == ModelType.MULTIMER:\n",
        "  all_chain_features = {}\n",
        "  for chain_id, chain_features in features_for_chain.items():\n",
        "    all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features(\n",
        "        chain_features, chain_id)\n",
        "\n",
        "  all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features)\n",
        "\n",
        "  np_example = feature_processing.pair_and_merge(\n",
        "      all_chain_features=all_chain_features)\n",
        "\n",
        "  # Pad MSA to avoid zero-sized extra_msa.\n",
        "  np_example = pipeline_multimer.pad_msa(np_example, min_num_seq=512)\n",
        "\n",
        "executed_cells.add(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8sC9oL96EIe"
      },
      "outputs": [],
      "source": [
        "#@title 5. Run AlphaFold and download embeddings\n",
        "\n",
        "#@markdown Once this cell has been executed, a zip-archive with\n",
        "#@markdown the obtained prediction will be automatically downloaded\n",
        "#@markdown to your computer.\n",
        "\n",
        "#@markdown In case you are having issues with the relaxation stage, you can disable it below.\n",
        "#@markdown Warning: This means that the prediction might have distracting\n",
        "#@markdown small stereochemical violations.\n",
        "\n",
        "#run_relax = False  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Relaxation is faster with a GPU, but we have found it to be less stable.\n",
        "#@markdown You may wish to enable GPU for higher performance, but if it doesn't\n",
        "#@markdown converge we suggested reverting to using without GPU.\n",
        "\n",
        "#relax_use_gpu = False  #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown The multimer model will continue recycling until the predictions stop\n",
        "#@markdown changing, up to the limit set here. For higher accuracy, at the\n",
        "#@markdown potential cost of longer inference times, set this to 20.\n",
        "\n",
        "multimer_model_max_num_recycles = 3  #@param {type:\"integer\"}\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 5)\n",
        "\n",
        "# --- Run the model ---\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  model_names = config.MODEL_PRESETS['monomer']\n",
        "elif model_type_to_use == ModelType.MULTIMER:\n",
        "  model_names = config.MODEL_PRESETS['multimer']\n",
        "\n",
        "output_dir = job_name\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "plddts = {}\n",
        "ranking_confidences = {}\n",
        "pae_outputs = {}\n",
        "unrelaxed_proteins = {}\n",
        "\n",
        "with tqdm.notebook.tqdm(total=len(model_names), bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "  for model_name in model_names:\n",
        "    pbar.set_description(f'Running {model_name}')\n",
        "\n",
        "    cfg = config.model_config(model_name)\n",
        "\n",
        "    if model_type_to_use == ModelType.MONOMER:\n",
        "      cfg.data.eval.num_ensemble = 1\n",
        "    elif model_type_to_use == ModelType.MULTIMER:\n",
        "      cfg.model.num_ensemble_eval = 1\n",
        "\n",
        "    if model_type_to_use == ModelType.MULTIMER:\n",
        "      cfg.model.num_recycle = multimer_model_max_num_recycles\n",
        "      cfg.model.recycle_early_stop_tolerance = 0.5\n",
        "\n",
        "    params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "    model_runner = model.RunModel(cfg, params)\n",
        "    processed_feature_dict = model_runner.process_features(np_example, random_seed=0)\n",
        "    prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize))\n",
        "    representation = prediction['representations']\n",
        "    output_dir_model = os.path.join(output_dir, f'{model_name}.npz')\n",
        "    np.savez(output_dir_model, single=representation['single'],\n",
        "                                  pair=representation['pair'])\n",
        "    #mean_plddt = prediction['plddt'].mean() stop point while modifying alphafold colab version.\n",
        "    mean_plddt = prediction['plddt'].mean()\n",
        "\n",
        "    if model_type_to_use == ModelType.MONOMER:\n",
        "      if 'predicted_aligned_error' in prediction:\n",
        "        pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                   prediction['max_predicted_aligned_error'])\n",
        "      else:\n",
        "        # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they\n",
        "        # should never get selected.\n",
        "        ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "        plddts[model_name] = prediction['plddt']\n",
        "    elif model_type_to_use == ModelType.MULTIMER:\n",
        "      # Multimer models are sorted by pTM+ipTM.\n",
        "      ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "      plddts[model_name] = prediction['plddt']\n",
        "      pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                 prediction['max_predicted_aligned_error'])\n",
        "    # Set the b-factors to the per-residue plddt.\n",
        "    final_atom_mask = prediction['structure_module']['final_atom_mask']\n",
        "    b_factors = prediction['plddt'][:, None] * final_atom_mask\n",
        "    unrelaxed_protein = protein.from_prediction(\n",
        "        processed_feature_dict,\n",
        "        prediction,\n",
        "        b_factors=b_factors,\n",
        "        remove_leading_feature_dimension=(\n",
        "            model_type_to_use == ModelType.MONOMER))\n",
        "    unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "\n",
        "    # Delete unused outputs to save memory.\n",
        "    del model_runner\n",
        "    del params\n",
        "    del prediction\n",
        "    pbar.update(n=1)\n",
        "\n",
        "    print(f'Running {model_name} successfully!')\n",
        "    unrelaxed_pdb = protein.to_pdb(unrelaxed_proteins[model_name])\n",
        "    banded_b_factors = []\n",
        "    for plddt in plddts[model_name]:\n",
        "      for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
        "        if plddt >= min_val and plddt <= max_val:\n",
        "          banded_b_factors.append(idx)\n",
        "          break\n",
        "    banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
        "    to_visualize_pdb = utils.overwrite_b_factors(unrelaxed_pdb, banded_b_factors)\n",
        "\n",
        "    # Write out the prediction\n",
        "    pred_output_path = os.path.join(output_dir, f'{model_name}.pdb')\n",
        "    with open(pred_output_path, 'w') as f:\n",
        "      f.write(unrelaxed_pdb)\n",
        "# --- Download the predictions ---\n",
        "shutil.make_archive(base_name=job_name, format='zip', root_dir=output_dir)\n",
        "files.download(f'{output_dir}.zip')\n",
        "\n",
        "executed_cells.add(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMhX8d6fSGzIjy7jdV/fFWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}